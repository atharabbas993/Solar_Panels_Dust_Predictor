{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM7bpN2uS0ru1hfqg+1Usa+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/atharabbas993/Solar_Panels_Dust_Predictor/blob/main/Solar_Panel_Dust_Detector.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. Import Libraries"
      ],
      "metadata": {
        "id": "p4kGZj1MDj8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "from keras.models import Sequential\n",
        "\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization, Flatten, Dense, Dropout\n",
        "\n",
        "from keras.callbacks import EarlyStopping"
      ],
      "metadata": {
        "id": "fO3loTyMgeD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "metadata": {
        "id": "IbDuN2Fdf3Tp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d hemanthsai7/solar-panel-dust-detection"
      ],
      "metadata": {
        "id": "_gHk1uYtf74y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Split the Data"
      ],
      "metadata": {
        "id": "lDWF_ASRMIwn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install split-folders"
      ],
      "metadata": {
        "id": "VBA8nPQDgAuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from zipfile import ZipFile\n",
        "\n",
        "with ZipFile('solar-panel-dust-detection.zip', 'r') as zipObj:\n",
        "   zipObj.extractall()\n"
      ],
      "metadata": {
        "id": "yAdFbr6CgDVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import splitfolders\n",
        "\n",
        "#splits the data into 80% for training, 10% for validation, and 10% for testing.\n",
        "\n",
        "splitfolders.ratio(\"/content/Detect_solar_dust\", output=\"output\", seed=1337, ratio=(.8, 0.1,0.1),group_prefix=None)\n"
      ],
      "metadata": {
        "id": "1Ox5k5NegF4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Data Preparation"
      ],
      "metadata": {
        "id": "_xOcFKjcDu5H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set paths to the dataset\n",
        "\n",
        "#Train data folder Directory\n",
        "\n",
        "train_dir = '/content/output/train'\n",
        "\n",
        "#Validation_Data_Directory\n",
        "\n",
        "validation_dir = '/content/output/val'\n",
        "\n",
        "#Test_Data_Directory\n",
        "\n",
        "test_dir = '/content/output/test'\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Image Data Generator for augmentation\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1.0/255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "validation_datagen = ImageDataGenerator(rescale=1.0/255)\n",
        "test_datagen = ImageDataGenerator(rescale=1.0/255)\n",
        "\n",
        "\n",
        "#____________Train_Data__________________\n",
        "\n",
        "# Load images from directories\n",
        "train_data = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(224, 224),  # Resize images\n",
        "    batch_size=32,\n",
        "    class_mode='binary'     # Binary classification\n",
        ")\n",
        "\n",
        "#___________Validation_Data__________________\n",
        "\n",
        "validation_data = validation_datagen.flow_from_directory(\n",
        "    validation_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "#_____________Test_Data________________________\n",
        "\n",
        "test_data = test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='binary'\n",
        ")\n"
      ],
      "metadata": {
        "id": "Pab6lsvMgKxP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Build the CNN Model"
      ],
      "metadata": {
        "id": "pLd5ZkEBD489"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "\n",
        "#_________Convalutional_Layers_______\n",
        "\n",
        "model.add(Conv2D(64, (3,3), activation='relu', input_shape=(224, 224, 3)))\n",
        "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv2D(128, (3,3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv2D(256, (3,3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "#______Flatten_Layer___\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "#______Fully_Connected_Layers______\n",
        "\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(1, activation='sigmoid'))\n"
      ],
      "metadata": {
        "id": "hDbJkpQwgTBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Architecture"
      ],
      "metadata": {
        "id": "RH8C8oGACyUB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils import plot_model\n",
        "\n",
        "plot_model(model, show_shapes=True, show_layer_names=True)"
      ],
      "metadata": {
        "id": "mzpDyr6jBkm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Summary"
      ],
      "metadata": {
        "id": "zDql6bfqJz9g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Shows the model's layer details and parameters.\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "sDtMcWSVBKBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. Compile the Model"
      ],
      "metadata": {
        "id": "h3C254SLEIl7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "89Fj3Agzg6FL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Implement Early_stopping"
      ],
      "metadata": {
        "id": "OWXspP8Y8oSZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "early_stopping = keras.callbacks.EarlyStopping(\n",
        "    monitor=\"val_loss\",\n",
        "    min_delta=0,\n",
        "    patience=3,\n",
        "    verbose=1,\n",
        "    mode=\"auto\",\n",
        "    baseline=None,\n",
        "    restore_best_weights=False,\n",
        "    start_from_epoch=0,\n",
        ")"
      ],
      "metadata": {
        "id": "hTpCxMQX7rqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. Train the Model\n"
      ],
      "metadata": {
        "id": "A_vvi-0jEObE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "history = model.fit(train_data, epochs=30, validation_data=validation_data, callbacks=[early_stopping])\n"
      ],
      "metadata": {
        "id": "h6Xk-Y7ahAtW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy = model.evaluate(test_data)\n",
        "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "t7Us5FRYsGrW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#8. Visualize Training Results (Optional)"
      ],
      "metadata": {
        "id": "CCTFLrANE0Op"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#_____________Visualize_Training_Results_________________\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot accuracy and loss over epochs\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ekSdJQvDE3rf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. Visualize Accuracy and Loss"
      ],
      "metadata": {
        "id": "djR2HWAMZmZD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Function to plot accuracy and loss\n",
        "def plot_training_history(history):\n",
        "    # Plot training and validation accuracy\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Model Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot training and validation loss\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'], label='Train Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Model Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Call the function\n",
        "plot_training_history(history)\n"
      ],
      "metadata": {
        "id": "o3eKawm3LTTl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Confusion Matrix for Test Results"
      ],
      "metadata": {
        "id": "2CTtAf_7Ztcu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
        "\n",
        "# Get true labels and predictions\n",
        "test_data.reset()  # Reset test data generator to prevent order mismatch\n",
        "y_true = test_data.classes\n",
        "y_pred = (model.predict(test_data) > 0.5).astype(\"int32\").flatten()\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=test_data.class_indices.keys())\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(8, 8))\n",
        "disp.plot(cmap='Blues', values_format='d')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report:\\n\", classification_report(y_true, y_pred, target_names=test_data.class_indices.keys()))\n"
      ],
      "metadata": {
        "id": "ET1umfZcLd34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. ROC Curve for Binary Classification"
      ],
      "metadata": {
        "id": "F6PxBkjrZ-sw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "# Get probabilities for ROC curve\n",
        "y_probs = model.predict(test_data).flatten()  # Get predicted probabilities\n",
        "fpr, tpr, thresholds = roc_curve(y_true, y_probs)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Plot ROC curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Random Guess')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ZA9KXJ-7LiYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. Precision-Recall Curve"
      ],
      "metadata": {
        "id": "0WduZDjvaKQk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "\n",
        "precision, recall, _ = precision_recall_curve(y_true, y_probs)\n",
        "avg_precision = average_precision_score(y_true, y_probs)\n",
        "\n",
        "# Plot Precision-Recall curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(recall, precision, label=f'Precision-Recall Curve (AP = {avg_precision:.2f})')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "H2P9N2-0aZx1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. Histogram of Predicted Probabilities"
      ],
      "metadata": {
        "id": "nQI8sOA5afkG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "\n",
        "# Get predicted probabilities\n",
        "y_probs = model.predict(test_data).flatten()\n",
        "\n",
        "# Plot histogram\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.histplot(y_probs, kde=True, bins=20, color='blue')\n",
        "plt.xlabel('Predicted Probability')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Histogram of Predicted Probabilities')\n",
        "plt.grid()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "cSq1OuzJLfHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6. Learning Rate Schedule"
      ],
      "metadata": {
        "id": "uRVVu5QUa97r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you logged learning rates during training\n",
        "learning_rates = [0.001, 0.0009, 0.0008, 0.0007, 0.0006]  # Example values\n",
        "\n",
        "# Plot learning rate over epochs\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(range(1, len(learning_rates) + 1), learning_rates, marker='o')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Learning Rate')\n",
        "plt.title('Learning Rate Schedule')\n",
        "plt.grid()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "IzU0qyx5VtDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#7. Training Time Per Epoch"
      ],
      "metadata": {
        "id": "Qa56XLigbKSZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "# If you tracked training time during each epoch:\n",
        "epoch_times = [5.2, 4.8, 5.1, 5.0, 5.3]  # Example times in seconds for each epoch\n",
        "\n",
        "# Plot training time per epoch\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(range(1, len(epoch_times) + 1), epoch_times, marker='o')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Time (seconds)')\n",
        "plt.title('Training Time Per Epoch')\n",
        "plt.grid()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ykf7E4MDV6Sc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#8. Class Distribution in the Dataset"
      ],
      "metadata": {
        "id": "45el5cCVbXSp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Count class instances\n",
        "class_counts = pd.DataFrame({\n",
        "    'Class': ['Clean', 'Dusty'],\n",
        "    'Count': [len(os.listdir('/content/output/train/Clean')), len(os.listdir('/content/output/train/Dusty'))]\n",
        "})\n",
        "\n",
        "# Bar plot of class distribution\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.barplot(x='Class', y='Count', data=class_counts, palette='coolwarm')\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Number of Images')\n",
        "plt.title('Class Distribution in Dataset')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Nph1SN2qWFLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#9. Model Performance Over Epochs with Log Scale"
      ],
      "metadata": {
        "id": "Gr3pN-4IbplQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.yscale('log')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss (Log Scale)')\n",
        "plt.title('Loss Over Epochs (Log Scale)')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "LvVfOBeYW3i9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#7. Save the Model"
      ],
      "metadata": {
        "id": "eHmUcMyDEcK4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#_____________Save_Model______________\n",
        "\n",
        "model.save('solar_panel_classifier.h5')\n"
      ],
      "metadata": {
        "id": "_q46T_GDsMkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8.Diploy the Model"
      ],
      "metadata": {
        "id": "ZZIPG_41Ejpt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#________________Diploy the Model_____________________\n",
        "\n",
        "from tensorflow.keras.models import load_model\n",
        "model = load_model('solar_panel_classifier.h5')\n",
        "\n",
        "# Make a prediction\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing import image\n",
        "\n",
        "# Load a single image for prediction\n",
        "img = image.load_img('/content/output/test/Dusty/Imgdirty_1054_1.jpg', target_size=(224, 224))\n",
        "img_array = image.img_to_array(img) / 255.0\n",
        "img_array = np.expand_dims(img_array, axis=0)\n",
        "\n",
        "# Predict\n",
        "prediction = model.predict(img_array)\n",
        "if prediction[0][0] > 0.5:\n",
        "    print(\"The solar panel is clean.\")\n",
        "else:\n",
        "    print(\"The solar panel is dusty.\")\n"
      ],
      "metadata": {
        "id": "vIJW0N-VsT5p"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}